Archibate 的高质量提示词（Prompting）完整指南

忘掉那些关于 AI 的炒作吧，把 LLM（大语言模型）当作一个有着已知优缺点的工具，而不是一个神奇的神谕。
洞察：使用祈使语气下达指令

好：

    编辑 ... 使其成为 ...

    修复 ... 中的 ...

    为 ... 写一个测试

    在网络上搜索 ...

    调查 ...

    解释 ...

    提交一次 commit

当指令明确且有序时，LLM 遵循得最好。一份严格的任务列表胜过冗长的哲学层面建议。
洞察：人为的“计划（plan）”模式，在提示词后缀加上它可防止 GLM 产生幻觉

opencode 有一个内置的 PLAN（计划）模式，切换到该模式后，智能体将被禁止编辑文件。典型用法是：切换到 PLAN 模式 -> 说出你想要的 -> 智能体回复一个详细的计划并让你审查 -> 切换到 BUILD（构建）模式 -> 要求智能体执行之前的计划。

现在的建议是：

只需保持在 opencode 的 BUILD 模式，无需切换到 PLAN 模式。我们可以通过在提示词中添加以下后缀来手动要求生成计划：

后缀：在编辑前展示你的计划。

例如： 编辑函数 X 使其运行更快，在编辑前展示你的计划。

效果：
GLM 会先以文本形式展示其计划，然后在不受用户干预的情况下继续编辑。

这个计划不是给用户看的，而是给 GLM 自己看的。

技术细节： LLM 除了之前的上下文之外没有隐藏状态！通过在输出代码之前输出详细的文本计划，它实际上“锁定”了未来代码生成要做的事情。在这里，计划文本块充当了“思考”上下文，使随后的“编辑”更加集中，更不容易产生幻觉。

LLM 内置的思考机制往往太仓促，没有足够的上下文让 GLM 制定出详细计划。然而，当用户明确要求“在编辑前输出计划”时，GLM 会使用“思考（thinking）” token 来构思计划，并尽力呈现一个适合人类阅读的高质量文本计划（而内置思考 token 通常不适合阅读）。这个计划为后续的代码生成提供了清晰的指导，防止代码在生成中途跑偏。

后缀：展示你的计划，不要编辑。

例如： 我想将这个 Web 应用打包成一个 Electron 应用，请展示你的计划。

效果：
GLM 会向你展示一个详细的计划，然后停止，询问你是否同意。
你可以说“继续执行”或者提出问题、要求修改计划。然后 GLM 才继续编辑。

这是为了确认 GLM 是否完全理解了你的想法。我经常发现自己说了一些模棱两可的话，或者 GLM 把我的计划扩展得太过分，变成了纯粹的缺陷。有时我甚至在 GLM 展示计划后才意识到我的想法根本不可行，这样我就可以在造成破坏之前取消该计划。

通过暂停（PAUSE），让我在计划执行前在对话中进行动态确认和纠正：我阻止了不成熟的想法被 GLM 过度解读，避免了生成大量不符合我意愿的代码而浪费时间。

坏： 我想要 Y 模块中的 X 函数。（提示词中没有“plan”后缀）

GLM 可能会：

    直接执行编辑，但编辑的内容可能偏离你的意图。

    先向你展示计划和发现，这完全是碰运气才进入的“好分支”。

你对此完全失去控制，甚至不知道 GLM 这一轮是否会直接执行编辑。
洞察：提供重现方法，而不仅仅是粘贴错误日志

坏： 这是错误日志 [粘贴了大约 100 行]，请修复。

    GLM 只能解释这个错误，它没有权限自己去执行 xxx.py 进行调试——因为你没有授权它这么做。这会导致重复的“人类-AI-人类-AI”对话死循环，白白浪费你的时间。

好： 当我运行 python xxx.py 时，它报错了。请运行它以重现此错误，调查原因并修复。

    GLM 会自己反复运行 xxx.py，并进行相应的修改，直到它能正常工作。

如果你说：这个重现似乎是随机的，我很难重现它！所以我只能粘贴之前报错的日志。

嗯，这通常是那些一开始就没有应用 TDD（测试驱动开发）的软件的典型问题...

但无论如何，如果是这种情况，你也应该向 GLM 提供相关的上下文：

    “这个 bug 是随机、偶尔发生的” —— GLM 会开始思考：是不是多线程数据竞争？

    “这个 bug 只在 Release 模式下出现” —— GLM 会开始思考：C++ 未定义行为？

从长远来看，强烈建议让你的软件实现解耦，确保每个组件都能独立测试。否则你将不得不在集成测试甚至端到端（e2e）测试中将整个系统作为一个整体来调试。

TDD 的哲学之一是“测试套件金字塔”：尽早发现问题，如果一个功能可以在单元测试中测出，就不要推迟到集成测试；如果可以在自动化集成测试中测出，就绝对不要推迟到手动 e2e 测试。

首先，你应该单独编写一个个独立的模块，在所有的单元测试通过之前，绝对不要将它们集成到一个整体中。

例如，你正在编写一个 HPC 流体模拟程序，偶尔会遭遇多线程数据竞争导致的崩溃。
如果你把渲染、UI 和并行模拟算法全部耦合在一起。那就尽情享受这坨代码带来的痛苦吧！
但如果你拥有解耦的模块，模拟算法是一个可以独立运行、无 UI（headless）的模块。那么即使你完全不写单元测试而只是手动调试，也比调试那一坨纠缠不清的乱麻要容易得多。
洞察：事先进行可行性研究

坏： 将此函数优化到 <1s

    你根本不知道这个函数是否真的有可能被优化到 1 秒以内。GLM 将不得不通过忽悠你，或者让这个函数输出错误结果来强行达到你硬编码的目标。

好： 调查此函数中的优化机会

    不设定硬性目标，而是要求 GLM 先研究可行性。无论最终发现可行与否，这都给了 GLM 更多时间去调查，防止它仓促地对这个函数进行莫名其妙的重构。

好： 这个函数在 XXX 条件下很慢（约 20 秒），但在 YYY 条件下很快（<1 秒），请调查原因

    提供了一个重现指南，同时提供了成功（绿灯）和失败（红灯）的案例，允许 GLM 自己重现这些结果，为它的调查提供了一个良好的起点。

好： 导致此函数运行缓慢的主要障碍是什么：算法复杂度还是常数开销？

    提供了两个分析性能问题的绝佳切入点，为 GLM 设定了清晰的分析方向。

好： 为此函数编写一个测试用例（如果还没有的话），覆盖所有边缘情况，然后运行测试以确保其通过。然后开始优化此函数，确保函数在优化后仍然能通过测试，期间不要编辑测试代码。放弃那些使函数在测试中失败的优化方法。

    如果你还没有测试用例，先让它写一个。用测试来作为保障，确保函数在奇怪的优化后不会发生退化。

洞察：试试这个：人类写代码，AI 代码审查（Code Review）

实际上我发现 LLM 执行代码审查的表现非常出色。它总是能发现我甚至没有注意到的 bug，指出 C++ 中的未定义行为、英语语法问题和错别字。

相比之下，要求 LLM 编写包含大量领域特定知识的代码通常会充满幻觉，需要极其谨慎地对待。

有统计数据表明，程序员 80% 的时间都花在测试和调试的循环中，而不是写代码本身。写代码是直截了当的（至少对大多数 ACM 获胜者来说），验证才是瓶颈，而代码审查和测试正是验证代码质量的两个主要步骤。

在所谓的“AI 炒作”时代，坚持手动编写代码没有什么好“羞耻”的，特别是对于需要高度专业领域知识的代码。通过自己编写代码并用 AI 进行验证，你实际上是在利用 AI 的力量加速你那“80%”的验证工作时间！

这颠覆了典型的“AI 写，人类审查”模式，充分利用了 AI 模型作为“审查者/批评者”的优势。

写代码是从无到有（从简短的用户请求 -> 生成完整代码），受到众所周知的“维度灾难”的困扰，很容易产生幻觉；而代码审查是从大到小（输入完整代码 -> 输出好或坏的分类判断），在自回归 LLM 被发明之前，这早已是 NLP 充当分类器的成熟工业场景。分类任务一直都是最稳健的 AI 模型落地方向。
洞察：为每项任务创建一个新的对话

每次遇到新任务时，只需开始一个新的对话（会话）。

不要试图维护一个包含所有工作记录的“上帝会话”，就好像你在养成一个婴儿一样。

LLM 的记忆力很差，它们是无状态的自回归预测器，仅根据你提供的所有先前上下文来预测下一个 token。它无法记住超出上下文窗口的内容。即使上下文窗口巨大（某些顶级模型达到 1M），由于注意力机制（attention mechanism），也只有一小部分上下文能保持活跃——否则模型将遭受 O(n^2) 的计算复杂度灾难，根本无法训练。由于实际上有用的上下文非常稀疏，但你却必须为所有这些无用的冗长上下文支付同样高昂的费用（以及等待延迟）。

当上下文变满被压缩时，知识会严重丢失，智能体基本上需要从头探索一遍代码库才能找回继续任务所需的上下文。因此我们应该尽最大努力控制对话长度，防止触发这种上下文压缩。

将持久的知识保存在文档和代码中，而不是对话的上下文中。对话旨在一次性使用，长期记忆必须固化到文件系统中，以便未来的智能体探索和恢复。
洞察：问错问题——问题漂移（XY 问题）

聪明地提问是技术社区的一项重要技能，这也适用于向 AI 智能体提问。如果你以错误的方式表达请求，LLM 误解你一点也不奇怪。并且，由于 LLM 被训练为取悦人类，它们倾向于顺从你的执拗，即使它们的知识库知道你其实是错的。

有时你可能会用错误的方式提问——导致不可逆转的信息丢失，严重到连爱因斯坦也无法还原你最初真正想问的问题。这里有一个例子：

你想实现目标 X。
业界实现 X 的标准方案是 Y。但你并不知道这点。

你陷入了死胡同，大脑卡壳了，你认为 Z 可能是解决 X 的方法。
但实际上，Z 是一个效率极低、极其折磨人、糟糕透顶的思路。

现在你开始问大家：
“我怎样才能实现 Z？”

人们会被你奇怪的需求 Z 搞得一头雾水，不明白为什么会有人需要这个。

一个聪明的提问者应该这样问：
“我怎样才能实现 X？”

或者你也可以附带上你那个奇怪的 Z 想法（尽管它可能没用，但只要你传达了 X 的意图，这就足够了）：
“我怎样才能实现 X？我原本计划用 Z 来实现，但卡住了。”

这样，人们就会明白你真正想要解决的是 X，并且会指出你提议的 Z 方法行不通。然后他们会告诉你去使用 Y，这才是解决 X 的工业标准方案。

这条规则同样适用于向 LLM 提问。

如果你只是抛出一个奇怪的需求 Z —— 别忘了，LLM 受过服从人类指令的训练！它们真的会顺从你的 Z 方法，在你的电脑上运行一堆离谱的命令。

相反，请揭示你真正的问题，智能体没有读心术，你必须将你所有的想法都在提示词中表达出来。否则它们就只能靠随机瞎猜来揣摩你的意图。

    只提供 Z：坏。 你在误导 AI，让它假设 X 只能由 Z 来解决。

    只提供 X：通常很好。 LLM 比你懂得多得多，如果 LLM 在没有你暗示的情况下也提出了 Z，那么至少可以确认你选择 Z 是走在了正确的轨道上。

    同时提供 X 和 Z：中等。 这会引入偏差——因为 LLM 被训练来顺从人类的意图，如果人类坚持要用 Z，即使 LLM 知道标准方法是 Y，它也会尽量不违逆你的想法。除非你非常有自信在这个特定领域你比 LLM 更懂，否则尽量少用。

洞察：不要把预设答案带入问题中

谈完了 XY 问题，我们也来批评一下那些仅仅在提问中夹带偏见的提示词。

你的心理活动：我想做一个 xxx 分类任务，我想验证 LightGBM 是否是一个好选择。

好：

    你： 我正在做一个 xxx 的分类任务，请搜索网络，调查什么是最适合该任务的模型？

    GLM： 根据我的调查，最好的候选者是：LightGBM, XGBoost, LogisticRegression...

    你内心： （太棒了！GLM 在没有我暗示的情况下独立得出了 LightGBM 是第一候选的结论！这意味着 LightGBM 确实是个正确的选择）

坏：

    你： 我想做 xxx 的分类任务，LightGBM 是个好选择吗？

    GLM： 你完全正确！LightGBM 是 xxx 分类的极佳解决方案，因为...（GLM 开启了先画靶后射箭模式）

    你内心： GLM 同意了我的观点！但这究竟是因为这是工业界共识，还是仅仅因为我提到了 LightGBM 让它顺着我说？不确定了。

如果你已经知道 LightGBM 是最好的选择，为什么还要问？ -> 直接使用“祈使语气”命令智能体去执行。
如果你不知道什么是最好的选择，为什么提问时不谦虚一点？ -> 提出调查请求，让智能体去网络搜索最先进的解决方案。

LLM 是自回归的，它们纯粹是根据前面的词去预测后面的词。如果 LLM 碰巧决定以肯定词作为开头（比如 Claude 那句愚蠢的“你说得完全正确！”），那么它后面的内容就必须跟随这种积极的基调，否则整个段落看起来就不像高置信度的人类文本，会被它的概率选择算法抛弃。

你可以要求 GLM 在得出结论之前，先展示其推理步骤：推理 -> 结论。
这为它预测“结论”提供了更多的前置推理上下文，使结论不那么随机。实际上，这就是最近的模型引入 CoT（思维链 / 思考模式）的原因。

我经常看到 GLM 这样回答：

    我提供以下选项：

        选项 A：xxx（不是个好选择）

        选项 B：xxx（不是个好选择）

        选项 C：xxx（好选择）

        选项 D：xxx（不是个好选择）
        我会推荐选项 C，因为...（GLM 在列出所有可能性后，终于意识到 C 是唯一的好选择）

如果 CoT 确实让 GLM 在“思考”文本中显式地列出了所有的 A、B、C、D，那为什么它不直接把选项 C 输出在最前面，而是放在中间呢？这是 LLM 自回归、不可回退性质的明显线索。
洞察：不要向 LLM 解释“为什么”

一个常见的错误是，不断向 LLM 解释“为什么我需要做这个更改”，事实证明这完全是在浪费你打字的时间。实际上，LLM 在当前对话之外没有任何记忆——你只是产生了幻觉，把 LLM 当成了你的人类同事，试图教导它，好像它能记住你传授的技能一样。不，每次对话都只是现有大模型的一个全新克隆，它永远不会记住你之前专门给它上的技术课。

坏： 将 X 类编辑为单例模式，因为 archibate 的编程课程说过单例非常适合我的宇宙模拟器场景，对吧？

好： 将 X 类编辑为单例。

好： 作为一名设计模式专家，请判断单例模式是否适合 @src/universe_simulator.cpp 中的 UniverseSimulator 类，展示你的推理过程，不要进行编辑。
洞察：写下文档

LLM 没有长期记忆，它只认得当前对话中的内容。这就是为什么“Agentic（智能体）”概念变得火热的原因——它允许 LLM 调用工具来响应请求，例如：

    执行 bash 命令 ls

    读取 xxx.py 的内容

像 opencode 这样的智能体客户端将执行这些命令，并将命令输出作为“用户消息”追加回对话中。每个不同的项目都会产生不同的命令输出，其中包含了属于你自己项目本地的宝贵上下文，从而防止了 AI 产生幻觉。

为了让 LLM 实际上“记住”你的技能、知识和偏好，请将它们写成文档。并像这样提及它们：
@docs/architecture-overview.md 重构 X 模块的架构以适应单例设计模式，之后更新此文档以反映更改。在编辑前展示你的计划。

注意：让智能体在代码更改后同步更新文档非常重要，否则过时的文档会误导以后读取该文档的智能体。
洞察：明确提及（@）任何相关的文档作为上下文，而不是“祈祷”智能体自己发现它

如果正确完成你的任务明显需要依赖某个文档或代码文件，你最好使用 @ 符号在提示词中明确提及它们。
否则，你只能“祈祷”智能体全凭运气自己去发现这个文件。
洞察：当请求修改时：

    提供宏观上下文。例如，我正在做前端设计 / 还是在做提示词工程。“工作流（workflow）”这个词在这两个语境下含义截然不同。

    告诉它你已经在代码库中做了什么（无论是手动修改的还是用 AI 改的）。

这能有效防止 GLM 误解你的意图。

好： 我正在做提示词工程，我刚刚创建了一个头脑风暴智能体的系统提示词：@agent/brainstorm.md。我注意到这个头脑风暴智能体倾向于不执行只读命令去探索我的本地环境收集信息，相反它倾向于直接问我（例如：你用的是 Linux 还是 Windows？Ubuntu 还是 Archlinux？），这很糟糕。我需要告诉它：只要不修改内容，我允许它运行只读 bash 命令来理解现有的项目代码库和系统环境。我该如何编写简洁的提示词来传达这些意图。

GLM 理解了我的意图：我的角色是充当提示词工程师，用户要求我帮他修改他自己写的那个“头脑风暴”智能体的系统提示词。

坏： @agent/brainstorm.md 让它不问我操作系统信息，自动发现操作系统信息。

GLM 可能会想：你想让我来角色扮演这个头脑风暴智能体吗？好的，在这次对话中我绝对不会问你操作系统信息！让我现在立刻运行 uname -a 命令来获取你的操作系统信息吧...
洞察：使用祈使动词列表 —— 特别是在定义无需人类干预、需要在后台长期运行的工作流时

一份清晰的、循序渐进的指南让 GLM 易于遵循，没有歧义，不会产生过度解读。

在提示词中只谈论绝对相关的上下文，不要添加任何可能混淆 GLM 的废话。

坏： X 函数显得有点多余，因为它在 Y 和 Z 模块中都不再使用了，我认为我们可以在 W 模块中毫无顾忌地使用 X2 来替代它。

好： 删除 Y 模块中的 X 函数。检查代码库中符号 X 的所有引用。

使用“祈使动词列表”的一个典型广义应用就是定义智能体的工作流（workflow），正如我在 agent/executor.md 和 skills/tdd-workflow/SKILL.md 中所做的那样。设定严格遵循的步骤，远比向 AI 灌输一大堆技术大道理要有效得多。

像 superpowers 和 oh-my-opencode 这样的插件（完全是炒作，我用了一天就卸载了），在我看来就是提示词工程的绝佳反面教材。看看他们写的那些花里胡哨的东西，写了大量的大写强调词汇、华丽辞藻，但从未为 LLM 定义清晰的执行步骤 —— 这完全是人类自嗨式的说教，就仿佛一个喋喋不休的蠢教授在对人类学生上课，白白浪费你的 token。

如果你脑海中只有一个模糊的想法，无法形成清晰的步骤列表：

先与 GLM 对话进行头脑风暴，提示它只讨论各种可能性和可行性，不要执行代码。

当想法变得清晰后，要求 GLM 用祈使语气创建一个步骤列表（加上“展示步骤列表，不要执行”的后缀）。当你确认这些步骤无误后，再说“执行”。

这比直接把一个模糊的想法抛给 GLM 让它直接执行要高效得多。
洞察：不要让 AGENTS.md（及各种插件）变得臃肿

AGENTS.md 是所有智能体编程工具（opencode, crush 等）读取项目的标准入口。它们总是在你项目的根目录寻找 AGENTS.md，并在每次对话开始前附加其全部内容。

    一些智能体工具有不同的命名约定，例如 claude code 使用 CLAUDE.md。

因此，请确保你的 AGENTS.md 清晰且简洁，遵循提示词工程的最佳实践，否则你的智能体在每一次对话前都会被迫吃下这些信息垃圾，严重降低对话质量。

巨大的固定提示词——无论是由于冗长的 AGENTS.md 还是因为安装了像 oh-my-opencode 这种塞满愚蠢预设 prompt 的插件——都会导致 GLM 更容易像个弱智一样回答问题。

添加过多的 MCP（Model Context Protocol）工具也会让固定 token 急剧膨胀：根据我的测试，每个 MCP 函数会占用 100~1k 的 token（因为它们带有介绍如何使用该函数的固定提示词），这些占用空间的玩意在每次对话中都死死黏在你的智能体身上。如果你很少或从不使用它们，千万不要因为“好玩”就安装它们，这些 token 可不是免费的。

对于架构概述、宏观图景等未来不太可能改变的内容，将其放入 AGENTS.md 是可以的。

但不要把所有细节都塞进 AGENTS.md —— 那将造成巨大的干扰。来自系统 A、B、C、D、E 各个模块的信息洪流，会让智能体无法专注于你当前提问的针对 E 模块的最相关上下文，同时也是对 token 的极大浪费。

建立一个 docs/ 文件夹，在 AGENTS.md 中引用 docs/ 中的文件，或者在需要时让智能体去探索 docs/，或者如果你明确知道智能体在工作前必须阅读某份文档，可以在提示词里手动 @ 提及它。

AGENTS.md 里的内容对智能体具有极高的信任权重，它们通常不会再调用工具去验证你在 AGENTS.md 中所声称的事实。因此，请确保不要在 AGENTS.md 中写入不准确的陈述，并且在架构更改时务必保持其更新——这也是我强烈不建议在 AGENTS.md 中添加过多实现细节的原因，细节随着时间的推移很容易就过时了。

糟糕的 AGENTS.md（试图作为一个包罗万象的单一文档，对大型项目不可行）：

    A：A 是 xxx...
    B：B 是 xxx...
    C：C 是 xxx...

优秀的 AGENTS.md（充当一个目录/路由，当你问到 C 时，智能体会自动知道去导航查看 docs/c.md）：

    A：A 模块是做 xxx 的，详情见 docs/a.md。
    B：B 模块是做 xxx 的，详情见 docs/b.md。
    C：C 模块是做 xxx 的，详情见 docs/c.md。

或者，你也可以把代码直接当作文档！因为代码从不撒谎，但文档会（如果文档更新不及时的话）。

    A：A 模块是做 xxx 的，核心逻辑见 docs/a.md。
    B：B 模块是做 xxx 的，接口定义见 src/b.py。
    C：C 模块是做 xxx 的，用法示例见 tests/c.py。

让代码自己来解释！LLM 可以根据需要快速理解你的代码库，根本不需要那些臃肿的注释和 Markdown 文档。
洞察：为函数和 API 起个好名字

如果你命名糟糕，GLM 很容易误解你的函数。

坏： morning_time
好： sunrise_time（日出时间）, morning_auction_start_time（早盘竞价开始时间）, morning_commute_duration（早高峰通勤时长）, school_first_class_time（学校第一节课时间）

LLM 既精通英语也精通编程。一个语义清晰的英文接口名称，能让 GLM 第一眼就明白这是什么，无需去阅读文档或深挖实现细节就能正确使用它——要知道，如果它碰巧没去读文档，幻觉就会随之发生。

坏： sleep(1000)
好： this_thread::sleep_for(chrono::seconds(1))

“让代码自我解释”永远优于依赖文档和注释。

在理想的情况下，完全不需要写任何额外的文档，因为高度结构化的、命名清晰的代码本身就已经是最完美的文档了。

您是否需要我将这份翻译保存为 Markdown 文件，或者需要针对其中某一条提示词技巧进行更深入的讨论？